{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическое задание к уроку 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Загрузим необходимые библиотеки и данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0   @user when a father is dysfunctional and is s...\n",
       "2       0  @user @user thanks for #lyft credit i can't us...\n",
       "3       0                                bihday your majesty\n",
       "4       0  #model   i love u take with u all the time in ...\n",
       "5       0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv', index_col='id')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание датасета:  \n",
    "The objective of this task is to detect hate speech in tweets.  \n",
    "For the sake of simplicity, we say a tweet contains hate speech  \n",
    "if it has a racist or sexist sentiment associated with it.  \n",
    "So, the task is to classify racist or sexist tweets from other tweets.  \n",
    "  \n",
    "Formally, given a training sample of tweets and labels, where label '1'  \n",
    "denotes the tweet is racist/sexist and label '0' denotes the tweet is  \n",
    "not racist/sexist, your objective is to predict the labels on the test dataset.  \n",
    "  \n",
    "Таким образом, нам нужно будет искать твиты, которые содержат  \n",
    "расистский или сексистский смысл.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17197, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv', index_col='id')\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как тестовые данные не содержат меток, то будем использовать только  \n",
    "трейн для обучения и валидации, чтобы можно было оценить качество модели.  \n",
    "Посмотрим на баланс классов:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    29720\n",
       "1     2242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.256021409455842"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()[0] / df_train['label'].value_counts()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем разбивку на трейн и валидацию:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 2), (6393, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=RANDOM_STATE, \n",
    "                                    stratify=df_train['label'])\n",
    "\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем подготовку текстов:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)\n",
    "# Не будем очищать текст от апострофов, заменим их потом на пробелы,\n",
    "# т.к. встроенные в nltk английские стопслова и так потом отфильтруют лишнее\n",
    "puncts = puncts - {\"'\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = ''.join(char for char in txt if char not in puncts) # очистка от пунктуации\n",
    "    txt = txt.replace(\"'\", \" \")\n",
    "    txt = txt.lower().split()\n",
    "    txt = [word for word in txt if word.isalpha()] # очистка от символов и цифр\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt] # лемматизация\n",
    "    txt = [word for word in txt if word not in stopwords.words('english')] # очистка от стопслов\n",
    "    return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25569/25569 [00:39<00:00, 652.12it/s]\n",
      "100%|██████████| 6393/6393 [00:09<00:00, 678.85it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14553</th>\n",
       "      <td>0</td>\n",
       "      <td>user amazing wait see going cantwait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>0</td>\n",
       "      <td>wait new user trailer gamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>0</td>\n",
       "      <td>thriving iam positive affirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>0</td>\n",
       "      <td>happy new user book lil upset page faded user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>arrive cold rainy english noh first time back ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "id                                                             \n",
       "14553      0               user amazing wait see going cantwait\n",
       "2563       0                        wait new user trailer gamer\n",
       "12125      0                  thriving iam positive affirmation\n",
       "6326       0  happy new user book lil upset page faded user ...\n",
       "3996       0  arrive cold rainy english noh first time back ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим общий корпус текста:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ''.join(df_train['tweet'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем токенизацию:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'amazing', 'wait', 'see', 'going']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 4000\n",
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens)\n",
    "tokens_top = [items[0] for items in dist.most_common(MAX_WORDS - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'day', 'love', 'u', 'amp', 'like', 'life', 'happy', 'get', 'wa']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {word: count for count, word in dict(enumerate(tokens_top, 1)).items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведём твиты в набор индексов, добавим паддинг:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_sequence(txt, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25569, 40), (6393, 40))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_train['tweet'].values])\n",
    "X_val = np.array([text_to_sequence(txt, MAX_LEN) for txt in df_val['tweet'].values])\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальная строка: found beautiful one bedroom double stall garage patio amp huge kitchen signed lease wait move\n",
      "Обработанная строка: [ 172   51   19 1233 3015 3475    5  777 1537 1538   68  694    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Оригинальная строка: {df_train['tweet'].iloc[5]}\")\n",
    "print(f\"Обработанная строка: {X_train[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем рекуррентную нейросеть:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=128, out_dim=64, use_last=True, threshold=0.5, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n",
    "        self.gru = nn.GRU(embedding_dim, out_dim, batch_first=True) \n",
    "        self.linear = nn.Linear(out_dim, num_classes)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        self.use_last = use_last\n",
    "        \n",
    "    def forward(self, x):                          \n",
    "        x = self.embedding(x)\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.gru(x)\n",
    "           \n",
    "        if self.use_last:\n",
    "            x = x[:,-1,:]\n",
    "        else:\n",
    "            x = torch.mean(x[:,:], dim=1)\n",
    "            \n",
    "        x = self.dp(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = torch.IntTensor(x).to(device)\n",
    "        x = self.forward(x)\n",
    "        x = torch.squeeze((x > self.threshold).int())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим структуру сети:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [1, 1]                    --\n",
       "├─Embedding: 1-1                         [1, 40, 128]              256,000\n",
       "├─Dropout: 1-2                           [1, 40, 128]              --\n",
       "├─GRU: 1-3                               [1, 40, 64]               37,248\n",
       "├─Dropout: 1-4                           [1, 64]                   --\n",
       "├─Linear: 1-5                            [1, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 293,313\n",
       "Trainable params: 293,313\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 1.17\n",
       "Estimated Total Size (MB): 1.23\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Net(), input_data=torch.IntTensor(X_train[np.newaxis, 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим датасеты:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(RANDOM_STATE)\n",
    "\n",
    "train_dataset = DataWrapper(X_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(X_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем код сети. Учитывая дисбаланс классов, метрика accuracy нам  \n",
    "не подходит. Вместо неё будем использовать F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(epochs=5, embedding_dim=128, hidden_size=64, lr=1e-3, threshold=0.5, use_last=True, return_model=False):\n",
    "\n",
    "    torch.random.manual_seed(RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    net = Net(vocab_size=MAX_WORDS, embedding_dim=embedding_dim, \n",
    "              out_dim=hidden_size, use_last=use_last, threshold=threshold).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = np.array([])\n",
    "        test_losses = np.array([])\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            net.train()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses = np.append(train_losses, loss.item())\n",
    "\n",
    "            net.eval()\n",
    "            outputs = torch.squeeze((net(inputs) > threshold).int())\n",
    "\n",
    "            tp += ((labels == 1) & (outputs == 1)).sum().item()\n",
    "            tn += ((labels == 0) & (outputs == 0)).sum().item()\n",
    "            fp += ((labels == 0) & (outputs == 1)).sum().item()\n",
    "            fn += ((labels == 1) & (outputs == 0)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "              f'Loss: {train_losses.mean():.3f}. ' \\\n",
    "              f'F1-score: {f1_score:.3f}', end='. ')\n",
    "\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "                test_losses = np.append(test_losses, loss.item())\n",
    "\n",
    "                tp += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                tn += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "                fp += ((labels == 0) & (torch.squeeze((outputs > threshold).int()) == 1)).sum()\n",
    "                fn += ((labels == 1) & (torch.squeeze((outputs > threshold).int()) == 0)).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        print(f'Test loss: {test_losses.mean():.3f}. Test F1-score: {f1_score:.3f}. Precision: {precision:.3f}. Recall: {recall:.3f}')\n",
    "\n",
    "    print('Training is finished!')\n",
    "    if return_model:\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель на 70 эпохах:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70]. Loss: 0.454. F1-score: 0.098. Test loss: 0.265. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [2/70]. Loss: 0.264. F1-score: 0.000. Test loss: 0.253. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [3/70]. Loss: 0.259. F1-score: 0.000. Test loss: 0.249. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [4/70]. Loss: 0.248. F1-score: 0.000. Test loss: 0.223. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [5/70]. Loss: 0.216. F1-score: 0.000. Test loss: 0.186. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [6/70]. Loss: 0.191. F1-score: 0.045. Test loss: 0.171. Test F1-score: 0.126. Precision: 1.000. Recall: 0.067\n",
      "Epoch [7/70]. Loss: 0.179. F1-score: 0.284. Test loss: 0.158. Test F1-score: 0.427. Precision: 0.807. Recall: 0.290\n",
      "Epoch [8/70]. Loss: 0.166. F1-score: 0.518. Test loss: 0.148. Test F1-score: 0.541. Precision: 0.745. Recall: 0.424\n",
      "Epoch [9/70]. Loss: 0.151. F1-score: 0.620. Test loss: 0.149. Test F1-score: 0.546. Precision: 0.745. Recall: 0.431\n",
      "Epoch [10/70]. Loss: 0.145. F1-score: 0.638. Test loss: 0.145. Test F1-score: 0.540. Precision: 0.750. Recall: 0.422\n",
      "Epoch [11/70]. Loss: 0.139. F1-score: 0.667. Test loss: 0.138. Test F1-score: 0.564. Precision: 0.766. Recall: 0.446\n",
      "Epoch [12/70]. Loss: 0.133. F1-score: 0.687. Test loss: 0.135. Test F1-score: 0.562. Precision: 0.779. Recall: 0.440\n",
      "Epoch [13/70]. Loss: 0.131. F1-score: 0.705. Test loss: 0.134. Test F1-score: 0.582. Precision: 0.724. Recall: 0.487\n",
      "Epoch [14/70]. Loss: 0.125. F1-score: 0.726. Test loss: 0.136. Test F1-score: 0.608. Precision: 0.704. Recall: 0.536\n",
      "Epoch [15/70]. Loss: 0.121. F1-score: 0.735. Test loss: 0.132. Test F1-score: 0.603. Precision: 0.780. Recall: 0.491\n",
      "Epoch [16/70]. Loss: 0.116. F1-score: 0.756. Test loss: 0.133. Test F1-score: 0.609. Precision: 0.749. Recall: 0.513\n",
      "Epoch [17/70]. Loss: 0.115. F1-score: 0.768. Test loss: 0.133. Test F1-score: 0.608. Precision: 0.775. Recall: 0.500\n",
      "Epoch [18/70]. Loss: 0.111. F1-score: 0.773. Test loss: 0.133. Test F1-score: 0.626. Precision: 0.706. Recall: 0.562\n",
      "Epoch [19/70]. Loss: 0.110. F1-score: 0.774. Test loss: 0.134. Test F1-score: 0.617. Precision: 0.767. Recall: 0.516\n",
      "Epoch [20/70]. Loss: 0.106. F1-score: 0.794. Test loss: 0.131. Test F1-score: 0.624. Precision: 0.779. Recall: 0.520\n",
      "Epoch [21/70]. Loss: 0.105. F1-score: 0.799. Test loss: 0.130. Test F1-score: 0.612. Precision: 0.784. Recall: 0.502\n",
      "Epoch [22/70]. Loss: 0.101. F1-score: 0.816. Test loss: 0.134. Test F1-score: 0.628. Precision: 0.776. Recall: 0.527\n",
      "Epoch [23/70]. Loss: 0.098. F1-score: 0.816. Test loss: 0.133. Test F1-score: 0.624. Precision: 0.784. Recall: 0.518\n",
      "Epoch [24/70]. Loss: 0.098. F1-score: 0.823. Test loss: 0.133. Test F1-score: 0.624. Precision: 0.766. Recall: 0.527\n",
      "Epoch [25/70]. Loss: 0.094. F1-score: 0.828. Test loss: 0.135. Test F1-score: 0.634. Precision: 0.802. Recall: 0.525\n",
      "Epoch [26/70]. Loss: 0.094. F1-score: 0.838. Test loss: 0.137. Test F1-score: 0.636. Precision: 0.808. Recall: 0.525\n",
      "Epoch [27/70]. Loss: 0.091. F1-score: 0.847. Test loss: 0.135. Test F1-score: 0.647. Precision: 0.797. Recall: 0.545\n",
      "Epoch [28/70]. Loss: 0.088. F1-score: 0.854. Test loss: 0.132. Test F1-score: 0.649. Precision: 0.789. Recall: 0.551\n",
      "Epoch [29/70]. Loss: 0.088. F1-score: 0.860. Test loss: 0.132. Test F1-score: 0.649. Precision: 0.762. Recall: 0.565\n",
      "Epoch [30/70]. Loss: 0.088. F1-score: 0.871. Test loss: 0.130. Test F1-score: 0.653. Precision: 0.804. Recall: 0.549\n",
      "Epoch [31/70]. Loss: 0.085. F1-score: 0.873. Test loss: 0.138. Test F1-score: 0.653. Precision: 0.786. Recall: 0.558\n",
      "Epoch [32/70]. Loss: 0.081. F1-score: 0.878. Test loss: 0.135. Test F1-score: 0.657. Precision: 0.754. Recall: 0.583\n",
      "Epoch [33/70]. Loss: 0.083. F1-score: 0.883. Test loss: 0.132. Test F1-score: 0.666. Precision: 0.773. Recall: 0.585\n",
      "Epoch [34/70]. Loss: 0.080. F1-score: 0.881. Test loss: 0.136. Test F1-score: 0.634. Precision: 0.853. Recall: 0.504\n",
      "Epoch [35/70]. Loss: 0.079. F1-score: 0.885. Test loss: 0.137. Test F1-score: 0.663. Precision: 0.755. Recall: 0.592\n",
      "Epoch [36/70]. Loss: 0.075. F1-score: 0.893. Test loss: 0.136. Test F1-score: 0.669. Precision: 0.812. Recall: 0.569\n",
      "Epoch [37/70]. Loss: 0.076. F1-score: 0.898. Test loss: 0.140. Test F1-score: 0.666. Precision: 0.765. Recall: 0.589\n",
      "Epoch [38/70]. Loss: 0.075. F1-score: 0.901. Test loss: 0.143. Test F1-score: 0.673. Precision: 0.764. Recall: 0.600\n",
      "Epoch [39/70]. Loss: 0.074. F1-score: 0.905. Test loss: 0.139. Test F1-score: 0.667. Precision: 0.745. Recall: 0.605\n",
      "Epoch [40/70]. Loss: 0.068. F1-score: 0.911. Test loss: 0.140. Test F1-score: 0.668. Precision: 0.774. Recall: 0.587\n",
      "Epoch [41/70]. Loss: 0.070. F1-score: 0.915. Test loss: 0.133. Test F1-score: 0.672. Precision: 0.753. Recall: 0.607\n",
      "Epoch [42/70]. Loss: 0.072. F1-score: 0.917. Test loss: 0.150. Test F1-score: 0.667. Precision: 0.783. Recall: 0.580\n",
      "Epoch [43/70]. Loss: 0.069. F1-score: 0.918. Test loss: 0.148. Test F1-score: 0.663. Precision: 0.790. Recall: 0.571\n",
      "Epoch [44/70]. Loss: 0.067. F1-score: 0.921. Test loss: 0.151. Test F1-score: 0.658. Precision: 0.806. Recall: 0.556\n",
      "Epoch [45/70]. Loss: 0.067. F1-score: 0.921. Test loss: 0.149. Test F1-score: 0.658. Precision: 0.737. Recall: 0.594\n",
      "Epoch [46/70]. Loss: 0.066. F1-score: 0.925. Test loss: 0.150. Test F1-score: 0.668. Precision: 0.754. Recall: 0.600\n",
      "Epoch [47/70]. Loss: 0.063. F1-score: 0.931. Test loss: 0.148. Test F1-score: 0.667. Precision: 0.760. Recall: 0.594\n",
      "Epoch [48/70]. Loss: 0.062. F1-score: 0.931. Test loss: 0.135. Test F1-score: 0.675. Precision: 0.771. Recall: 0.600\n",
      "Epoch [49/70]. Loss: 0.061. F1-score: 0.936. Test loss: 0.148. Test F1-score: 0.668. Precision: 0.743. Recall: 0.607\n",
      "Epoch [50/70]. Loss: 0.063. F1-score: 0.936. Test loss: 0.152. Test F1-score: 0.673. Precision: 0.770. Recall: 0.598\n",
      "Epoch [51/70]. Loss: 0.064. F1-score: 0.939. Test loss: 0.142. Test F1-score: 0.665. Precision: 0.759. Recall: 0.592\n",
      "Epoch [52/70]. Loss: 0.060. F1-score: 0.940. Test loss: 0.150. Test F1-score: 0.656. Precision: 0.743. Recall: 0.587\n",
      "Epoch [53/70]. Loss: 0.059. F1-score: 0.941. Test loss: 0.152. Test F1-score: 0.659. Precision: 0.734. Recall: 0.598\n",
      "Epoch [54/70]. Loss: 0.059. F1-score: 0.945. Test loss: 0.159. Test F1-score: 0.667. Precision: 0.745. Recall: 0.605\n",
      "Epoch [55/70]. Loss: 0.056. F1-score: 0.946. Test loss: 0.159. Test F1-score: 0.667. Precision: 0.753. Recall: 0.598\n",
      "Epoch [56/70]. Loss: 0.057. F1-score: 0.949. Test loss: 0.158. Test F1-score: 0.670. Precision: 0.754. Recall: 0.603\n",
      "Epoch [57/70]. Loss: 0.056. F1-score: 0.952. Test loss: 0.148. Test F1-score: 0.668. Precision: 0.757. Recall: 0.598\n",
      "Epoch [58/70]. Loss: 0.057. F1-score: 0.951. Test loss: 0.160. Test F1-score: 0.670. Precision: 0.780. Recall: 0.587\n",
      "Epoch [59/70]. Loss: 0.057. F1-score: 0.952. Test loss: 0.158. Test F1-score: 0.663. Precision: 0.799. Recall: 0.567\n",
      "Epoch [60/70]. Loss: 0.054. F1-score: 0.952. Test loss: 0.165. Test F1-score: 0.665. Precision: 0.767. Recall: 0.587\n",
      "Epoch [61/70]. Loss: 0.054. F1-score: 0.955. Test loss: 0.164. Test F1-score: 0.658. Precision: 0.732. Recall: 0.598\n",
      "Epoch [62/70]. Loss: 0.054. F1-score: 0.957. Test loss: 0.169. Test F1-score: 0.661. Precision: 0.691. Recall: 0.634\n",
      "Epoch [63/70]. Loss: 0.054. F1-score: 0.960. Test loss: 0.154. Test F1-score: 0.667. Precision: 0.764. Recall: 0.592\n",
      "Epoch [64/70]. Loss: 0.054. F1-score: 0.961. Test loss: 0.163. Test F1-score: 0.666. Precision: 0.758. Recall: 0.594\n",
      "Epoch [65/70]. Loss: 0.053. F1-score: 0.960. Test loss: 0.156. Test F1-score: 0.667. Precision: 0.753. Recall: 0.598\n",
      "Epoch [66/70]. Loss: 0.050. F1-score: 0.963. Test loss: 0.166. Test F1-score: 0.670. Precision: 0.754. Recall: 0.603\n",
      "Epoch [67/70]. Loss: 0.050. F1-score: 0.967. Test loss: 0.158. Test F1-score: 0.668. Precision: 0.743. Recall: 0.607\n",
      "Epoch [68/70]. Loss: 0.051. F1-score: 0.965. Test loss: 0.171. Test F1-score: 0.664. Precision: 0.736. Recall: 0.605\n",
      "Epoch [69/70]. Loss: 0.048. F1-score: 0.966. Test loss: 0.165. Test F1-score: 0.664. Precision: 0.765. Recall: 0.587\n",
      "Epoch [70/70]. Loss: 0.048. F1-score: 0.970. Test loss: 0.167. Test F1-score: 0.676. Precision: 0.766. Recall: 0.605\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(epochs=70, use_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель GRU показала результат лучше, чем 1D свёртки из прошлого практического  \n",
    "задания, F1-score выше где-то на 8%. Также она оказалась чуть лучше, чем модель  \n",
    "LSTM (выявлено в результате перебора гиперпараметров). По логам видно, что  \n",
    "переобучение снова присутствует. Интересно, что модель GRU положительно отреагировала  \n",
    "на увеличение размера словаря и длины последовательности, тогда как свёрточная сеть  \n",
    "никак на это не реагировала.  \n",
    "Финальную модель обучим на 30 эпохах, где у нас относительно малый тест лосс  \n",
    "и относительно высокая метрика. Так же, как и в прошлый раз, снизим порог  \n",
    "классификации для получения более высокого Recall, который важен в нашей задаче:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]. Loss: 0.454. F1-score: 0.124. Test loss: 0.265. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [2/30]. Loss: 0.264. F1-score: 0.000. Test loss: 0.253. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [3/30]. Loss: 0.259. F1-score: 0.000. Test loss: 0.249. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [4/30]. Loss: 0.248. F1-score: 0.000. Test loss: 0.223. Test F1-score: 0.000. Precision: 0.000. Recall: 0.000\n",
      "Epoch [5/30]. Loss: 0.216. F1-score: 0.327. Test loss: 0.186. Test F1-score: 0.454. Precision: 0.522. Recall: 0.402\n",
      "Epoch [6/30]. Loss: 0.191. F1-score: 0.543. Test loss: 0.171. Test F1-score: 0.515. Precision: 0.524. Recall: 0.507\n",
      "Epoch [7/30]. Loss: 0.179. F1-score: 0.602. Test loss: 0.158. Test F1-score: 0.543. Precision: 0.545. Recall: 0.540\n",
      "Epoch [8/30]. Loss: 0.166. F1-score: 0.637. Test loss: 0.148. Test F1-score: 0.574. Precision: 0.596. Recall: 0.554\n",
      "Epoch [9/30]. Loss: 0.151. F1-score: 0.659. Test loss: 0.149. Test F1-score: 0.584. Precision: 0.621. Recall: 0.551\n",
      "Epoch [10/30]. Loss: 0.145. F1-score: 0.691. Test loss: 0.145. Test F1-score: 0.598. Precision: 0.670. Recall: 0.540\n",
      "Epoch [11/30]. Loss: 0.139. F1-score: 0.714. Test loss: 0.138. Test F1-score: 0.600. Precision: 0.661. Recall: 0.549\n",
      "Epoch [12/30]. Loss: 0.133. F1-score: 0.734. Test loss: 0.135. Test F1-score: 0.616. Precision: 0.684. Recall: 0.560\n",
      "Epoch [13/30]. Loss: 0.131. F1-score: 0.747. Test loss: 0.134. Test F1-score: 0.626. Precision: 0.628. Recall: 0.623\n",
      "Epoch [14/30]. Loss: 0.125. F1-score: 0.757. Test loss: 0.136. Test F1-score: 0.637. Precision: 0.630. Recall: 0.645\n",
      "Epoch [15/30]. Loss: 0.121. F1-score: 0.772. Test loss: 0.132. Test F1-score: 0.629. Precision: 0.673. Recall: 0.592\n",
      "Epoch [16/30]. Loss: 0.116. F1-score: 0.788. Test loss: 0.133. Test F1-score: 0.634. Precision: 0.657. Recall: 0.612\n",
      "Epoch [17/30]. Loss: 0.115. F1-score: 0.800. Test loss: 0.133. Test F1-score: 0.635. Precision: 0.672. Recall: 0.603\n",
      "Epoch [18/30]. Loss: 0.111. F1-score: 0.809. Test loss: 0.133. Test F1-score: 0.650. Precision: 0.625. Recall: 0.676\n",
      "Epoch [19/30]. Loss: 0.110. F1-score: 0.814. Test loss: 0.134. Test F1-score: 0.639. Precision: 0.654. Recall: 0.625\n",
      "Epoch [20/30]. Loss: 0.106. F1-score: 0.820. Test loss: 0.131. Test F1-score: 0.645. Precision: 0.679. Recall: 0.614\n",
      "Epoch [21/30]. Loss: 0.105. F1-score: 0.826. Test loss: 0.130. Test F1-score: 0.639. Precision: 0.680. Recall: 0.603\n",
      "Epoch [22/30]. Loss: 0.101. F1-score: 0.834. Test loss: 0.134. Test F1-score: 0.640. Precision: 0.672. Recall: 0.612\n",
      "Epoch [23/30]. Loss: 0.098. F1-score: 0.836. Test loss: 0.133. Test F1-score: 0.644. Precision: 0.695. Recall: 0.600\n",
      "Epoch [24/30]. Loss: 0.098. F1-score: 0.847. Test loss: 0.133. Test F1-score: 0.654. Precision: 0.675. Recall: 0.634\n",
      "Epoch [25/30]. Loss: 0.094. F1-score: 0.852. Test loss: 0.135. Test F1-score: 0.650. Precision: 0.694. Recall: 0.612\n",
      "Epoch [26/30]. Loss: 0.094. F1-score: 0.862. Test loss: 0.137. Test F1-score: 0.658. Precision: 0.715. Recall: 0.609\n",
      "Epoch [27/30]. Loss: 0.091. F1-score: 0.860. Test loss: 0.135. Test F1-score: 0.647. Precision: 0.671. Recall: 0.625\n",
      "Epoch [28/30]. Loss: 0.088. F1-score: 0.869. Test loss: 0.132. Test F1-score: 0.652. Precision: 0.663. Recall: 0.641\n",
      "Epoch [29/30]. Loss: 0.088. F1-score: 0.874. Test loss: 0.132. Test F1-score: 0.652. Precision: 0.650. Recall: 0.654\n",
      "Epoch [30/30]. Loss: 0.088. F1-score: 0.879. Test loss: 0.130. Test F1-score: 0.650. Precision: 0.671. Recall: 0.629\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "my_net = train_nn(epochs=30, threshold=0.25, return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сравнению с предыдущим решением на свёртках, мы находим почти столько же  \n",
    "оскорбительных твитов. Но показатель точности теперь выше на 21%.  \n",
    "В общем, эти результаты всё равно являются не очень хорошими для  \n",
    "готовой модели. Снова будем считать, что основной причиной является недостаток данных  \n",
    "(25 тысяч примеров в обучающей выборке, из которых всего 1800 положительного  \n",
    "класса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    23775\n",
       "1     1794\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание модели:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.predict(X_val[np.newaxis, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
